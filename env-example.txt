# ==========================================
# IDP Environment Configuration Template
# ==========================================
# Copy this file to .env and adjust values.
# DO NOT commit your actual .env file to version control.

# --- Application Settings ---
# Environment: dev | prod
APP_ENV=dev
APP_VERSION=0.1.0
REGION=local
# Tenant ID for multi-tenancy (default to a UUID)
TENANT_ID=00000000-0000-0000-0000-000000000001

# --- Database (PostgreSQL) ---
# Must match values in docker-compose.yml
POSTGRES_DB=ragdb
POSTGRES_USER=rag
# SECURITY WARNING: Change this in production!
POSTGRES_PASSWORD=change-me-strong

# --- Object Storage (MinIO) ---
# Must match values in docker-compose.yml
MINIO_ROOT_USER=ragminio
# SECURITY WARNING: Change this in production!
MINIO_ROOT_PASSWORD=change-me-strong
S3_BUCKET=rag-blobs
S3_CANONICAL_BUCKET=rag-canonical

# Public endpoint for presigned URLs generated by API for the browser.
# For local dev: http://localhost:9000
# For prod: https://s3.your-domain.com
S3_PUBLIC_ENDPOINT=http://localhost:9000

# --- Vector Database (Qdrant) ---
# Leave empty if not using authentication (default for local docker)
QDRANT_API_KEY=

# --- API Security ---
# Secures the /v1/chat/completions and management endpoints.
# If empty, authentication might be disabled (dev only).
IDP_API_KEY=

# --- LLM Providers & Generation ---
# You must set at least one API key or configure a local model.

# OpenAI (GPT-4, etc.)
OPENAI_API_KEY=

# Gemini (Google)
GEMINI_API_KEY=

# Generation Model Configuration
# To use OpenAI:
# GEN_BASE_URL=https://api.openai.com/v1/
# GEN_MODEL=gpt-4-turbo

# To use Gemini (via OpenAI compat):
# GEN_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai/
# GEN_MODEL=gemini-2.0-flash

# To use Ollama (Local):
# GEN_BASE_URL=http://host.docker.internal:11434/v1/
# GEN_MODEL=llama3

# Default active configuration (Gemini Example):
GEN_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai/
GEN_MODEL=gemini-2.0-flash

# --- Embedding Model ---
# Options: text-embedding-3-large (OpenAI), or local huggingface path
# Optional local embedding model (used when OPENAI_API_KEY is not set)
EMBED_LOCAL_MODEL=BAAI/bge-m3

# --- Ingestion Behavior ---
# Reject files with disallowed extensions or oversize filenames?
INGEST_STRICT_MODE=false

# --- CORS ---
# Comma-separated list of allowed origins.
# Example: http://localhost:5173,https://myapp.com
CORS_ALLOW_ORIGINS=

# --- Advanced / MinerU (OCR) ---
MINERU_HOME=/models/mineru
MINERU_DOWNLOAD_MODELS=true
PARSE_METHOD=auto
PARSE_AUTO_OCR_FALLBACK=true
PARSE_SPARSE_TEXT_THRESHOLD=400

# --- Generation Tweaks ---
# Force JSON output for answers? 1=Yes, 0=No
GEN_REQUIRE_JSON=1
# Require footnote citations [^1]?
GEN_REQUIRE_FOOTNOTES=1